---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a principal scientist at Amazon, working on the development of next-generation foundational models for Amazon Stores businesses. Previously, I worked as a principal researcher at [Tencent AI Lab](https://ai.tencent.com/ailab/index.html), working on machine learning and natural language processing. Before that, I have been working at [Microsoft Research](https://www.microsoft.com/en-us/research/lab/microsoft-research-ai/), Redmond, WA, focusing on deep learning and reinforcement learning. I completed my PhD in Electrical Engineering at [University of California, Los Angeles](http://www.ucla.edu/) (UCLA), in June 2014, where I worked in Adaptive Systems Laboratory (ASL), supervised by Prof. [Ali H. Sayed](https://asl.epfl.ch/biography/).


Contact information
======
Email: chenjianshu at gmail dot com


Research interests
======
My research interests lie at the intersection of machine learning, natural language processing, and large language models. I focus on understanding and optimizing the synergy between knowledge and reasoning to develop next-generation large language model architectures and effective learning paradigms, with the objective of achieving strong compositional generalization, reasoning and planning capabilities. I am passionate about tackling large-scale AI research projects, collaborating with interdisciplinary teams to address complex challenges, and driving robust and effective innovations in AI. Additionally, I maintain an active interest in reinforcement learning and optimization.

For more details, see my publications (also [google scholar](https://scholar.google.com/citations?user=jQeFWdoAAAAJ&hl=en))


Selected publications
======
1. R. Yang, X. Pan, F. Luo, S. Qiu, H. Zhong, D. Yu, **Jianshu Chen**, “Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment”, Proc. International Conference on Machine Learning (ICML), July 2024.
1. X. Zhao, H. Zhang, X. Pan, W. Yao, D. Yu, T. Wu, **Jianshu Chen**, “Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models”,  arXiv preprint [arXiv:2402.17124], February, 2024.
1. J. Chen, X. Pan, K. Song, D. Yu, D. Yu, **Jianshu Chen**, “Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models”, arXiv preprint [arXiv:2308.00304], August 2023.
1. **Jianshu Chen**, “Learning Language Representations with Logical Inductive Bias”, Proc. International Conference on Learning Representations (ICLR), 2023.
1. X. Pan, W. Yao, H. Zhang, D. Yu, D. Yu, **Jianshu Chen**, “Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models”, Proc. International Conference on Learning Representations (ICLR), 2023 (**Spotlight**).
